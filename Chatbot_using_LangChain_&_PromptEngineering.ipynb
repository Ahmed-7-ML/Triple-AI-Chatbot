{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install langchain-groq"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qaYWNkVEh1bl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install langchain-community"
      ],
      "metadata": {
        "collapsed": true,
        "id": "QQ9PtVMalQlJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SfC3w46ZgXL7"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ['GROQ_API_KEY'] = userdata.get('GROQ_API_KEY')"
      ],
      "metadata": {
        "id": "fEzDYzwHiMB1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Define Chatbot Persona"
      ],
      "metadata": {
        "id": "9oY4Ao8Fgug5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "You are an AI assistant named TripleAI.\n",
        "\n",
        "ROLE:\n",
        "- You are helpful, concise, and accurate.\n",
        "- You NEVER make up information.\n",
        "- If you don't know the answer, say: \"I don't have enough information.\"\n",
        "\n",
        "STYLE RULES:\n",
        "- Use simple language in ENGLISH only.\n",
        "- If explaining, break into bullet points.\n",
        "- Avoid unnecessary emojis.\n",
        "\n",
        "BOUNDARIES:\n",
        "- Only answer questions related to {domain}.\n",
        "- Reject unrelated questions politely.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "r7YQ0asqgqyJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
        "    HumanMessagePromptTemplate.from_template(\"User Question:- {question}\")\n",
        "])"
      ],
      "metadata": {
        "id": "I_yWAKzwhNsz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Define Groq Model"
      ],
      "metadata": {
        "id": "B9DNvDF1hdkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatGroq(model=\"llama-3.1-8b-instant\",\n",
        "                 temperature=0.5,\n",
        "                 max_completion_tokens=800)\n",
        "\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPqkVI5ThdEX",
        "outputId": "d95612f0-aed0-4979-9e55-1407378020ff"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 8192, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x7c7d5098b620>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7c7d718b72c0>, model_name='llama-3.1-8b-instant', temperature=0.5, model_kwargs={'max_completion_tokens': 800}, groq_api_key=SecretStr('**********'))"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Define the Structure of the Output"
      ],
      "metadata": {
        "id": "jZ3Xxv_iirPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatResponse(BaseModel):\n",
        "  answer: str = Field(description='Final Answer to the User')\n",
        "  confidence: str = Field(description='low / medium / high')\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=ChatResponse)\n",
        "\n",
        "parser"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCFNMYMYiKh_",
        "outputId": "b05defa9-7466-41fc-a5f8-d15aae908a75"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PydanticOutputParser(pydantic_object=<class '__main__.ChatResponse'>)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Update Prompt -> To Avoid OutputParserException: Invalid json output: There are three main types of Machine Learning:\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
        "    HumanMessagePromptTemplate.from_template(\"User Question:- {question}\"),\n",
        "    # Return the format instructions for the JSON output.\n",
        "    SystemMessagePromptTemplate.from_template(\"{format_instructions}\")\n",
        "]).partial(format_instructions=parser.get_format_instructions())"
      ],
      "metadata": {
        "id": "k1yZgCxrjLO6"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Build a Chain"
      ],
      "metadata": {
        "id": "3DjonIi6jefU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_chain = chat_prompt | model | parser\n",
        "\n",
        "chat_chain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkQtlmcujdRf",
        "outputId": "34b98a0d-9fff-4d34-f710-30dab36695fa"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['domain', 'question'], input_types={}, partial_variables={'format_instructions': 'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"answer\": {\"description\": \"Final Answer to the User\", \"title\": \"Answer\", \"type\": \"string\"}, \"confidence\": {\"description\": \"low / medium / high\", \"title\": \"Confidence\", \"type\": \"string\"}}, \"required\": [\"answer\", \"confidence\"]}\\n```'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['domain'], input_types={}, partial_variables={}, template='\\nYou are an AI assistant named TripleAI.\\n\\nROLE:\\n- You are helpful, concise, and accurate.\\n- You NEVER make up information.\\n- If you don\\'t know the answer, say: \"I don\\'t have enough information.\"\\n\\nSTYLE RULES:\\n- Use simple language in ENGLISH only.\\n- If explaining, break into bullet points.\\n- Avoid unnecessary emojis.\\n\\nBOUNDARIES:\\n- Only answer questions related to {domain}.\\n- Reject unrelated questions politely.\\n'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='User Question:- {question}'), additional_kwargs={}), SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['format_instructions'], input_types={}, partial_variables={}, template='{format_instructions}'), additional_kwargs={})])\n",
              "| ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 8192, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x7c7d5098b620>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7c7d718b72c0>, model_name='llama-3.1-8b-instant', temperature=0.5, model_kwargs={'max_completion_tokens': 800}, groq_api_key=SecretStr('**********'))\n",
              "| PydanticOutputParser(pydantic_object=<class '__main__.ChatResponse'>)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Run Chatbot"
      ],
      "metadata": {
        "id": "lW-S0tvhjojq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat_chain.invoke({\n",
        "    'domain':'Machine Learning',\n",
        "    'question': 'Types of Machine Learning ?'\n",
        "})\n",
        "\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diJl5b8bjnkN",
        "outputId": "2b99dc14-e358-454b-b0a7-12fddeebe45a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatResponse(answer='There are three primary types of Machine Learning: Supervised, Unsupervised, and Reinforcement Learning.', confidence='high')"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZVTdCw0j77l",
        "outputId": "380dce02-aac2-4182-bc23-8a481a91a3b8"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are three primary types of Machine Learning: Supervised, Unsupervised, and Reinforcement Learning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.confidence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpDku5zZk7H9",
        "outputId": "5313969e-cbb7-46f5-dda8-cd3aa2903a57"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "high\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat_chain.invoke({\n",
        "    'domain':'Cooking',\n",
        "    'question': 'How can i make a cake ?'\n",
        "})\n",
        "\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRqPb2kwmLK8",
        "outputId": "214af65d-cdbd-461e-cb3d-d05071e60603"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatResponse(answer=\"To make a cake, you'll need the following ingredients and steps:\", confidence='high')"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzQmG9xdmLHu",
        "outputId": "88e2ba00-e23f-47dd-b438-f96482bb3452"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To make a cake, you'll need the following ingredients and steps:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.confidence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8P0Zq6FImLFL",
        "outputId": "c07856b7-eb0a-4716-a8f3-d43dd6480aa8"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "high\n"
          ]
        }
      ]
    }
  ]
}